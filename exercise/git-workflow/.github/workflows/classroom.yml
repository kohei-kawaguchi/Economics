name: Autograding Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read
  issues: read
  pull-requests: write
  checks: write

jobs:
  code-correctness:
    name: Code Correctness (50 points)
    runs-on: ubuntu-latest
    outputs:
      result: ${{ steps.test.outcome }}
      details: ${{ steps.test.outputs.details }}
      points: ${{ steps.test.outputs.points }}
      passed: ${{ steps.test.outputs.passed }}
      total: ${{ steps.test.outputs.total }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run code tests
        id: test
        continue-on-error: true
        run: |
          set +e
          OUTPUT=$(uv run pytest tests/test_analysis.py --junitxml=pytest-code.xml -v 2>&1)
          EXIT_CODE=$?
          echo "$OUTPUT"
          
          # Save output for feedback
          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "details<<$EOF" >> $GITHUB_OUTPUT
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          python - <<'PY'
          import os
          import xml.etree.ElementTree as ET

          max_points = 50

          tree = ET.parse("pytest-code.xml")
          root = tree.getroot()

          tests = 0
          failures = 0
          errors = 0
          skipped = 0
          for ts in root.iter("testsuite"):
              tests += int(ts.attrib.get("tests", 0))
              failures += int(ts.attrib.get("failures", 0))
              errors += int(ts.attrib.get("errors", 0))
              skipped += int(ts.attrib.get("skipped", 0))

          passed = tests - failures - errors - skipped
          if tests <= 0:
              raise ValueError("No tests found in JUnit report.")

          points = (max_points * passed) // tests

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"passed={passed}\n")
              f.write(f"total={tests}\n")
              f.write(f"points={points}\n")
          PY
          
          exit $EXIT_CODE

      - name: Run analysis script
        if: steps.test.outcome == 'success'
        run: uv run python scripts/run_analysis.py

  git-workflow:
    name: Git Workflow (50 points)
    runs-on: ubuntu-latest
    outputs:
      result: ${{ steps.test.outcome }}
      details: ${{ steps.test.outputs.details }}
      points: ${{ steps.test.outputs.points }}
      passed: ${{ steps.test.outputs.passed }}
      total: ${{ steps.test.outputs.total }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run workflow tests
        id: test
        continue-on-error: true
        run: |
          set +e
          OUTPUT=$(uv run pytest tests/test_workflow.py --junitxml=pytest-workflow.xml -v 2>&1)
          EXIT_CODE=$?
          echo "$OUTPUT"
          
          # Save output for feedback
          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "details<<$EOF" >> $GITHUB_OUTPUT
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          python - <<'PY'
          import os
          import xml.etree.ElementTree as ET

          max_points = 50

          tree = ET.parse("pytest-workflow.xml")
          root = tree.getroot()

          tests = 0
          failures = 0
          errors = 0
          skipped = 0
          for ts in root.iter("testsuite"):
              tests += int(ts.attrib.get("tests", 0))
              failures += int(ts.attrib.get("failures", 0))
              errors += int(ts.attrib.get("errors", 0))
              skipped += int(ts.attrib.get("skipped", 0))

          passed = tests - failures - errors - skipped
          if tests <= 0:
              raise ValueError("No tests found in JUnit report.")

          points = (max_points * passed) // tests

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"passed={passed}\n")
              f.write(f"total={tests}\n")
              f.write(f"points={points}\n")
          PY
          
          exit $EXIT_CODE
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          GH_TOKEN: ${{ github.token }}

  feedback:
    name: Post Feedback
    runs-on: ubuntu-latest
    needs: [code-correctness, git-workflow]
    if: always() && github.event_name == 'pull_request'
    steps:
      - name: Generate feedback comment
        id: feedback
        run: |
          CODE_POINTS="${{ needs.code-correctness.outputs.points }}"
          CODE_PASSED="${{ needs.code-correctness.outputs.passed }}"
          CODE_TOTAL="${{ needs.code-correctness.outputs.total }}"
          WORKFLOW_POINTS="${{ needs.git-workflow.outputs.points }}"
          WORKFLOW_PASSED="${{ needs.git-workflow.outputs.passed }}"
          WORKFLOW_TOTAL="${{ needs.git-workflow.outputs.total }}"
          
          if [ "$CODE_POINTS" -eq 50 ]; then
            CODE_EMOJI="âœ…"
          else
            CODE_EMOJI="âŒ"
          fi

          if [ "$WORKFLOW_POINTS" -eq 50 ]; then
            WORKFLOW_EMOJI="âœ…"
          else
            WORKFLOW_EMOJI="âŒ"
          fi
          
          TOTAL=$((CODE_POINTS + WORKFLOW_POINTS))
          
          # Build comment
          cat << 'COMMENT_EOF' > comment.md
          ## ðŸ“Š Autograding Results
          
          | Component | Status | Points | Tests |
          |-----------|--------|--------|-------|
          COMMENT_EOF
          
          echo "| Code Correctness | $CODE_EMOJI | $CODE_POINTS/50 | $CODE_PASSED/$CODE_TOTAL |" >> comment.md
          echo "| Git Workflow | $WORKFLOW_EMOJI | $WORKFLOW_POINTS/50 | $WORKFLOW_PASSED/$WORKFLOW_TOTAL |" >> comment.md
          echo "| Total |  | $TOTAL/100 |  |" >> comment.md
          echo "" >> comment.md
          
          # Add feedback based on results
          if [ "$CODE_POINTS" -ne 50 ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Code correctness issues
          
          The `calculate_mean` function is not working correctly. Make sure:
          
          - The function returns the arithmetic mean (sum divided by count)
          - It handles the input list correctly
          - Check the test output in the Actions tab for details
          
          Hint: `return sum(numbers) / len(numbers)`
          
          FEEDBACK_EOF
          fi
          
          if [ "$WORKFLOW_POINTS" -ne 50 ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Git workflow issues
          
          Some workflow tests did not pass. Check the test output in the Actions tab for details.
          
          FEEDBACK_EOF
          fi
          
          if [ "$TOTAL" == "100" ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### ðŸŽ‰ Excellent Work!
          
          All tests passed. You have successfully:
          - Implemented the `calculate_mean` function correctly
          - Followed proper git workflow practices
          - Written good commit messages
          - Used issues and pull requests effectively
          
          FEEDBACK_EOF
          fi
          
          echo "" >> comment.md
          echo "---" >> comment.md
          echo "*This is an automated review. Check the Actions tab for detailed test output.*" >> comment.md

      - name: Post comment to PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('comment.md', 'utf8');
            
            // Find existing bot comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(c => 
              c.user.type === 'Bot' && c.body.includes('Autograding Results')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
