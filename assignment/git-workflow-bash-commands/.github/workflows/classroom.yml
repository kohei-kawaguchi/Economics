name: Autograding Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read
  issues: read
  pull-requests: write
  checks: write

jobs:
  bash-tasks:
    name: Bash Tasks (40 points)
    runs-on: ubuntu-latest
    outputs:
      result: ${{ steps.test.outcome }}
      details: ${{ steps.test.outputs.details }}
      points: ${{ steps.test.outputs.points }}
      passed: ${{ steps.test.outputs.passed }}
      total: ${{ steps.test.outputs.total }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run bash task tests
        id: test
        continue-on-error: true
        run: |
          set +e
          OUTPUT=$(uv run pytest tests/test_bash_tasks.py --junitxml=pytest-bash.xml -v 2>&1)
          EXIT_CODE=$?
          echo "$OUTPUT"

          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "details<<$EOF" >> $GITHUB_OUTPUT
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          python - <<'PY'
          import os
          import xml.etree.ElementTree as ET

          max_points = 40

          tree = ET.parse("pytest-bash.xml")
          root = tree.getroot()

          tests = 0
          failures = 0
          errors = 0
          skipped = 0
          for ts in root.iter("testsuite"):
              tests += int(ts.attrib.get("tests", 0))
              failures += int(ts.attrib.get("failures", 0))
              errors += int(ts.attrib.get("errors", 0))
              skipped += int(ts.attrib.get("skipped", 0))

          passed = tests - failures - errors - skipped
          if tests <= 0:
              raise ValueError("No tests found in JUnit report.")

          points = (max_points * passed) // tests

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"passed={passed}\n")
              f.write(f"total={tests}\n")
              f.write(f"points={points}\n")
          PY

          exit $EXIT_CODE

  code-correctness:
    name: Code Correctness (30 points)
    runs-on: ubuntu-latest
    outputs:
      result: ${{ steps.test.outcome }}
      details: ${{ steps.test.outputs.details }}
      points: ${{ steps.test.outputs.points }}
      passed: ${{ steps.test.outputs.passed }}
      total: ${{ steps.test.outputs.total }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run code tests
        id: test
        continue-on-error: true
        run: |
          set +e
          OUTPUT=$(uv run pytest tests/test_analysis.py --junitxml=pytest-code.xml -v 2>&1)
          EXIT_CODE=$?
          echo "$OUTPUT"

          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "details<<$EOF" >> $GITHUB_OUTPUT
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          python - <<'PY'
          import os
          import xml.etree.ElementTree as ET

          max_points = 30

          tree = ET.parse("pytest-code.xml")
          root = tree.getroot()

          tests = 0
          failures = 0
          errors = 0
          skipped = 0
          for ts in root.iter("testsuite"):
              tests += int(ts.attrib.get("tests", 0))
              failures += int(ts.attrib.get("failures", 0))
              errors += int(ts.attrib.get("errors", 0))
              skipped += int(ts.attrib.get("skipped", 0))

          passed = tests - failures - errors - skipped
          if tests <= 0:
              raise ValueError("No tests found in JUnit report.")

          points = (max_points * passed) // tests

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"passed={passed}\n")
              f.write(f"total={tests}\n")
              f.write(f"points={points}\n")
          PY

          exit $EXIT_CODE

      - name: Run analysis script
        if: steps.test.outcome == 'success'
        run: uv run python scripts/run_analysis.py

  git-workflow:
    name: Git Workflow (30 points)
    runs-on: ubuntu-latest
    outputs:
      result: ${{ steps.test.outcome }}
      details: ${{ steps.test.outputs.details }}
      points: ${{ steps.test.outputs.points }}
      passed: ${{ steps.test.outputs.passed }}
      total: ${{ steps.test.outputs.total }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run workflow tests
        id: test
        continue-on-error: true
        run: |
          set +e
          OUTPUT=$(uv run pytest tests/test_workflow.py --junitxml=pytest-workflow.xml -v 2>&1)
          EXIT_CODE=$?
          echo "$OUTPUT"

          EOF=$(dd if=/dev/urandom bs=15 count=1 status=none | base64)
          echo "details<<$EOF" >> $GITHUB_OUTPUT
          echo "$OUTPUT" >> $GITHUB_OUTPUT
          echo "$EOF" >> $GITHUB_OUTPUT

          python - <<'PY'
          import os
          import xml.etree.ElementTree as ET

          max_points = 30

          tree = ET.parse("pytest-workflow.xml")
          root = tree.getroot()

          tests = 0
          failures = 0
          errors = 0
          skipped = 0
          for ts in root.iter("testsuite"):
              tests += int(ts.attrib.get("tests", 0))
              failures += int(ts.attrib.get("failures", 0))
              errors += int(ts.attrib.get("errors", 0))
              skipped += int(ts.attrib.get("skipped", 0))

          passed = tests - failures - errors - skipped
          if tests <= 0:
              raise ValueError("No tests found in JUnit report.")

          points = (max_points * passed) // tests

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"passed={passed}\n")
              f.write(f"total={tests}\n")
              f.write(f"points={points}\n")
          PY

          exit $EXIT_CODE
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          GH_TOKEN: ${{ github.token }}

  feedback:
    name: Post Feedback
    runs-on: ubuntu-latest
    needs: [bash-tasks, code-correctness, git-workflow]
    if: always() && github.event_name == 'pull_request'
    steps:
      - name: Generate feedback comment
        run: |
          BASH_POINTS="${{ needs.bash-tasks.outputs.points }}"
          BASH_PASSED="${{ needs.bash-tasks.outputs.passed }}"
          BASH_TOTAL="${{ needs.bash-tasks.outputs.total }}"
          CODE_POINTS="${{ needs.code-correctness.outputs.points }}"
          CODE_PASSED="${{ needs.code-correctness.outputs.passed }}"
          CODE_TOTAL="${{ needs.code-correctness.outputs.total }}"
          WORKFLOW_POINTS="${{ needs.git-workflow.outputs.points }}"
          WORKFLOW_PASSED="${{ needs.git-workflow.outputs.passed }}"
          WORKFLOW_TOTAL="${{ needs.git-workflow.outputs.total }}"

          if [ "$BASH_POINTS" -eq 40 ]; then
            BASH_EMOJI="âœ…"
          else
            BASH_EMOJI="âŒ"
          fi

          if [ "$CODE_POINTS" -eq 30 ]; then
            CODE_EMOJI="âœ…"
          else
            CODE_EMOJI="âŒ"
          fi

          if [ "$WORKFLOW_POINTS" -eq 30 ]; then
            WORKFLOW_EMOJI="âœ…"
          else
            WORKFLOW_EMOJI="âŒ"
          fi

          TOTAL=$((BASH_POINTS + CODE_POINTS + WORKFLOW_POINTS))

          cat << 'COMMENT_EOF' > comment.md
          ## ðŸ“Š Autograding Results

          | Component | Status | Points | Tests |
          |-----------|--------|--------|-------|
          COMMENT_EOF

          echo "| Bash Tasks | $BASH_EMOJI | $BASH_POINTS/40 | $BASH_PASSED/$BASH_TOTAL |" >> comment.md
          echo "| Code Correctness | $CODE_EMOJI | $CODE_POINTS/30 | $CODE_PASSED/$CODE_TOTAL |" >> comment.md
          echo "| Git Workflow | $WORKFLOW_EMOJI | $WORKFLOW_POINTS/30 | $WORKFLOW_PASSED/$WORKFLOW_TOTAL |" >> comment.md
          echo "| Total |  | $TOTAL/100 |" >> comment.md
          echo "" >> comment.md

          if [ "$BASH_POINTS" -ne 40 ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Bash tasks issues

          Make sure you:

          - Create sandbox/notes/commands.txt with the required command lines
          - Create sandbox/scripts/hello.sh with the required contents
          - Make hello.sh executable with chmod +x
          - Edit sandbox/notes/vim_exercise.txt to match the expected contents
          - Add the PATH export and sourcing lines in sandbox/dotfiles

          FEEDBACK_EOF
          fi

          if [ "$CODE_POINTS" -ne 30 ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Code correctness issues

          Implement calculate_trimmed_mean in src/analysis.py so it returns the trimmed mean based on trim_ratio.

          FEEDBACK_EOF
          fi

          if [ "$WORKFLOW_POINTS" -ne 30 ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Git workflow issues

          Some workflow tests did not pass. Check the test output in the Actions tab for details.

          FEEDBACK_EOF
          fi

          if [ "$TOTAL" == "100" ]; then
            cat << 'FEEDBACK_EOF' >> comment.md
          ### Excellent work

          All tests passed.

          FEEDBACK_EOF
          fi

          echo "" >> comment.md
          echo "---" >> comment.md
          echo "*This is an automated review. Check the Actions tab for detailed test output.*" >> comment.md

      - name: Post comment to PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('comment.md', 'utf8');

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Autograding Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

